---
title: "Crimedataregression"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos=structure(c(CRAN="YOUR FAVORITE MIRROR")))
install.packages("outliers")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("caret")
library(caret)
library(tidyverse)
library(dplyr)
library(outliers)
```
Using Regression analysis to predict observed crime rate. The data set includes
the effect of punishment regimes on crime rates in the United states on 47 states
in the year of 1960.

```{r}
crime <- read.delim("http://www.statsci.org/data/general/uscrime.txt")
head(crime)

```
The data set contains the following variables:

Variable   Description
M       percentage of males aged 14-24 in total state population
So      indicator variable for a southern state
Ed      mean years of schooling of the population aged 25 years or over
Po1     per capita expenditure on police protection in 1960
Po2     per capita expenditure on police protection in 1959
LF      labour force participation rate of civilian urban males in the age-group 14-24
M.F     number of males per 100 females
Pop     state population in 1960 in hundred thousands
NW      percentage of nonwhites in the population
U1      unemployment rate of urban males 14-24
U2      unemployment rate of urban males 35-39
Wealth  wealth: median value of transferable assets or family income
Ineq    income inequality: percentage of families earning below half the median income
Prob    probability of imprisonment: ratio of number of commitments to number of offenses
Time    average time in months served by offenders in state prisons before their first release
Crime       crime rate: number of offenses per 100,000 population in 1960


```{r}
a <- data.frame(M = 14.0,So = 0,Ed = 10.0, Po1 = 12.0,Po2 = 15.5,
                 LF = 0.640, M.F = 94.0,Pop = 150,NW = 1.1,U1 = 0.120,
                 U2 = 3.6, Wealth = 3200,Ineq = 20.1,Prob = 0.04, Time = 39.0)

#This is the test data for regression model

```
A linear regression is a statistical model that will analyze relationship between response variables compared to other variables. The goal is to find relevant variables to build a model that can be as accurate as possible in order to predict crime rate.

Below is the line creating the linear regression model. 

```{r}
model1 <- lm(Crime~., data = crime)

```


```{r}
plot(model1)

```
You can see from the plot there a couple outliers but not hugely influential in data set.
Looking at the model we can get an idea for accuracy from the R squared value and influential variables to have a p value less than 0.05. Lets predict a value from using this model we created.

```{r}
predict(model1, a)
min(crime$Crime) # minimum value in response variable crime
```
Predicting a value using this model we see that through linear regression that this value is way out of the range of data. It is far below the minimum. This model most likely is not going to predict accurately. 

```{r}
summary(model1)

```
Looking at the summary we can see what features have a higher significant impact on the response variable. This is determined by p value. We can just use variable with a p value less than 0.05. 

The null hypothesis is basically that the predictor is not meaningful in the model. So if the p value is less than 0.05, we reject null and conclude that the variable is significant. 

Below in the model we have only included variables that are significant.

```{r}
model2 <- lm(Crime~M+Ed+Ineq+Prob, data = crime)
summary(model2)
```
Looking at the summary of model 2 we now see that Ed and Prob are the only variables with p value less than 0.05. Lets see what model 2 predicts.

```{r}

predict(model2, a)

```
This is the prediction for model 2. Lets look at the max value and see if the value is in range.
```{r}
print(max(crime$Crime)) #max is 1993
print(min(crime$Crime)) #min is 342
```
Model 2 value is within the range so the predicted value makes sense. Looking at R squared values between model1 and model2 you can see model1 has a better R squared value at 0.7 while model 2 is at 0.19. The higher the R squared value, it shows the model fits the observed data better. Since we are taking all variables into account in model1, this is most likely overfitting. 

```{r}

model3 <- lm(Crime~M+Ed+Ineq+Prob+Po1+U2, data = crime)
summary(model3)

```
For model 3 I added in Po1 and U2 and I picked these two variables to be added since in the initial model 1 they were the two variables that still had a p value above 0.05 but only very slightly compared to other variables. Looking at the R squared value for model 3, we can see it is 0.7 which is pretty good so this model fits the data pretty well. 

```{r}

predict(model3, a)

```
Model 3 gives a predicted value of 1304 which is within the range of values so it this data point does make sense. 


```{r}
AIC(model1)
AIC(model2)
AIC(model3)
```
We can also see that the AIC is smallest for model 3 which shows its the best model compared to the others.


```{r}
crossvalidate <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

model4 <- train(Crime~M+Ed+Ineq+Prob+Po1+U2, data = crime, method = "lm", trControl = crossvalidate)

print(model4)


```
We can also use K-fold Cross validation to further validate data and reduce bias. We get a R squared with 0.75 which is also an improved R squared value.This model predicts the best according to R2 value considering overfitting is minimized and using feature selection.
